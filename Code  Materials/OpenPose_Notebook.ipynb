{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HcTUTt_mXzWD","outputId":"f5f21c20-17a1-4a79-84aa-2f6ce5320a15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (3.5.2)\n","Requirement already satisfied: pyparsing>=2.2.1 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (1.22.4)\n","Requirement already satisfied: cycler>=0.10 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pillow>=6.2.0 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (9.1.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (1.4.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (4.33.3)\n","Requirement already satisfied: six>=1.5 in /Users/viduth/miniforge3/envs/mlp/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install matplotlib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEYi__uCYHG6","executionInfo":{"status":"ok","timestamp":1658299935606,"user_tz":-330,"elapsed":24211,"user":{"displayName":"Viduth Ishara","userId":"17395933505961235457"}},"outputId":"f6977910-75de-4beb-a84a-aae8f4992a22"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBXG29oZXzWI"},"outputs":[],"source":["import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bz44JfILXzWK"},"outputs":[],"source":["imagepath = \"/Users/viduth/Desktop/output\"\n","imgarray = os.listdir(imagepath)\n"]},{"cell_type":"markdown","metadata":{"id":"v2DGSsYWXzWK"},"source":["## Specify the model to be used\n","COCO and MPI are body pose estimation model. COCO has 18 points and MPI has 15 points as output.\n","\n","HAND is hand keypoints estimation model. It has 22 points as output\n","\n","Ensure that the model files are available in the folders."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9op-0LJFXzWN"},"outputs":[],"source":["MODE = \"COCO\"\n","\n","if MODE == \"COCO\":\n","    protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"\n","    weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"\n","    nPoints = 18\n","    POSE_PAIRS = [ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n","\n","elif MODE == \"MPI\" :\n","    protoFile = \"pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n","    weightsFile = \"pose/mpi/pose_iter_160000.caffemodel\"\n","    nPoints = 15\n","    POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cfF5B0JXzWO"},"outputs":[],"source":["net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n","\n","inWidth = 368\n","inHeight = 368"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4yk5n-YXzWQ","outputId":"1af55d1d-554a-46f6-da7b-f47026abc8c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["frame597.jpg\n","frame226.jpg\n","frame232.jpg\n","frame636.jpg\n","frame637.jpg\n","frame233.jpg\n","frame227.jpg\n","frame231.jpg\n","frame225.jpg\n","frame543.jpg\n","frame635.jpg\n","frame609.jpg\n","frame608.jpg\n","frame634.jpg\n","frame224.jpg\n","frame230.jpg\n","frame234.jpg\n","frame546.jpg\n","frame220.jpg\n","frame68.jpg\n","frame630.jpg\n",".DS_Store\n",".DS_Store is deffective\n","frame368.jpg\n","frame369.jpg\n","frame631.jpg\n","frame69.jpg\n","frame547.jpg\n","frame221.jpg\n","frame235.jpg\n","frame223.jpg\n","frame545.jpg\n","frame551.jpg\n","frame237.jpg\n","frame633.jpg\n","frame357.jpg\n","frame356.jpg\n","frame632.jpg\n","frame550.jpg\n","frame236.jpg\n","frame222.jpg\n","frame544.jpg\n","frame245.jpg\n","frame279.jpg\n","frame641.jpg\n","frame640.jpg\n","frame278.jpg\n","frame244.jpg\n","frame246.jpg\n","frame642.jpg\n","frame643.jpg\n","frame247.jpg\n","frame243.jpg\n","frame242.jpg\n","frame268.jpg\n","frame240.jpg\n","frame650.jpg\n","frame651.jpg\n","frame241.jpg\n","frame269.jpg\n","frame270.jpg\n","frame271.jpg\n","frame273.jpg\n","frame272.jpg\n","frame276.jpg\n","frame277.jpg\n","frame275.jpg\n","frame274.jpg\n","frame549.jpg\n","frame603.jpg\n","frame67.jpg\n","frame73.jpg\n","frame373.jpg\n","frame367.jpg\n","frame366.jpg\n","frame372.jpg\n","frame72.jpg\n","frame66.jpg\n","frame602.jpg\n","frame548.jpg\n","frame238.jpg\n","frame600.jpg\n","frame70.jpg\n","frame64.jpg\n","frame364.jpg\n","frame370.jpg\n","frame358.jpg\n","frame359.jpg\n","frame371.jpg\n","frame365.jpg\n","frame629.jpg\n","frame65.jpg\n","frame71.jpg\n","frame601.jpg\n","frame239.jpg\n","frame598.jpg\n","frame229.jpg\n","frame75.jpg\n","frame639.jpg\n","frame61.jpg\n","frame605.jpg\n","frame611.jpg\n","frame361.jpg\n","frame375.jpg\n","frame374.jpg\n","frame360.jpg\n","frame610.jpg\n","frame604.jpg\n","frame638.jpg\n","frame74.jpg\n","frame228.jpg\n","frame599.jpg\n","frame62.jpg\n","frame606.jpg\n","frame362.jpg\n","frame363.jpg\n","frame607.jpg\n","frame63.jpg\n"]}],"source":["for count,img in enumerate(imgarray):\n","    print(img)\n","    try:\n","        frame = cv2.imread(f'{imagepath}/{img}')\n","        frameCopy = np.copy(frame)\n","        frameWidth = frame.shape[1]\n","        frameHeight = frame.shape[0]\n","        threshold = 0.1\n","\n","        inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n","                              (0, 0, 0), swapRB=False, crop=False)\n","\n","        net.setInput(inpBlob)\n","\n","        output = net.forward()\n","        H = output.shape[2]\n","        W = output.shape[3]\n","\n","            # Empty list to store the detected keypoints\n","        points = []\n","\n","        blankframe = np.zeros([frameWidth,frameHeight,1],dtype=np.uint8)\n","        blankframe.fill(255)\n","        \n","        for i in range(nPoints):\n","            # confidence map of corresponding body's part.\n","            probMap = output[0, i, :, :]\n","\n","            # Find global maxima of the probMap.\n","            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n","\n","            # Scale the point to fit on the original image\n","            x = (frameWidth * point[0]) / W\n","            y = (frameHeight * point[1]) / H\n","\n","            if prob > threshold : \n","                cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n","                cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n","                cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n","                cv2.circle(blankframe, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n","\n","                # Add the point to the list if the probability is greater than the threshold\n","                points.append((int(x), int(y)))\n","            else :\n","                points.append(None)\n","\n","        # Draw Skeleton\n","        for pair in POSE_PAIRS:\n","            partA = pair[0]\n","            partB = pair[1]\n","\n","            if points[partA] and points[partB]:\n","                cv2.line(blankframe, points[partA], points[partB], (0, 255, 255), 3)\n","                cv2.line(frame, points[partA], points[partB], (0, 255, 255), 3)\n","\n","    #     plt.figure(figsize=[10,10])\n","    #     plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))\n","    #     plt.figure(figsize=[10,10])\n","    #     plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","        cv2.imwrite(f'/Users/viduth/Desktop/ns/image.{count}.png', cv2.cvtColor(blankframe, cv2.COLOR_BGR2RGB))\n","        cv2.imwrite(f'/Users/viduth/Desktop/no/image.{count}.png', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","    except:\n","        print(f'{img} is deffective')"]},{"cell_type":"markdown","metadata":{"id":"-FZELj-aXzWT"},"source":["#### Let us load an image with multiple people and check what the model sees"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee2z2XA8XzWV"},"outputs":[],"source":["# image1 = cv2.imread(\"multiple.jpeg\")\n","# frameWidth = image1.shape[1]\n","# frameHeight = image1.shape[0]\n","# threshold = 0.1"]},{"cell_type":"markdown","metadata":{"id":"-I-mnCFHXzWW"},"source":["#### Load the network and pass the image through the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5vvF4N6XzWX"},"outputs":[],"source":["# net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n","\n","# inWidth = 368\n","# inHeight = 368\n","# inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n","#                           (0, 0, 0), swapRB=False, crop=False)\n","\n","# net.setInput(inpBlob)\n","# output = net.forward()\n","# H = output.shape[2]\n","# W = output.shape[3]\n","# print(output.shape)"]},{"cell_type":"markdown","metadata":{"id":"VT7NmTGiXzWX"},"source":["#### Slice a probability map from the output for a specific keypoint and plot the heatmap ( after resizing ) on the image itself"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqpqx4hhXzWY"},"outputs":[],"source":["# i = 5\n","# probMap = output[0, i, :, :]\n","# probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n","# plt.figure(figsize=[14,10])\n","# plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n","# plt.imshow(probMap, alpha=0.6)\n","# plt.colorbar()\n","# plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"o_TEbYMvXzWY"},"source":["#### Similarly plot the affinity map on the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZo9clRrXzWZ"},"outputs":[],"source":["# i = 24\n","# probMap = output[0, i, :, :]\n","# probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n","# plt.figure(figsize=[14,10])\n","# plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n","# plt.imshow(probMap, alpha=0.6)\n","# plt.colorbar()\n","# plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"bSkDFNr7XzWZ"},"source":["#### Next, we find the keypoints for a image with only single person"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8KSyKjuXzWZ"},"outputs":[],"source":["# frame = cv2.imread(\"frame159.jpg\")\n","# frameCopy = np.copy(frame)\n","# frameWidth = frame.shape[1]\n","# frameHeight = frame.shape[0]\n","# threshold = 0.1"]},{"cell_type":"markdown","metadata":{"id":"6rUSu7ZRXzWa"},"source":["#### Pass it through the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXnhAJqRXzWa"},"outputs":[],"source":["# inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n","#                           (0, 0, 0), swapRB=False, crop=False)\n","\n","# net.setInput(inpBlob)\n","\n","# output = net.forward()\n","# H = output.shape[2]\n","# W = output.shape[3]"]},{"cell_type":"markdown","metadata":{"id":"vtiGmYLSXzWa"},"source":["#### gather the points and plot the keypoints and the skeleton figure"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"E2GqVJpnXzWb"},"outputs":[],"source":["# # Empty list to store the detected keypoints\n","# points = []\n","\n","# for i in range(nPoints):\n","#     # confidence map of corresponding body's part.\n","#     probMap = output[0, i, :, :]\n","\n","#     # Find global maxima of the probMap.\n","#     minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n","    \n","#     # Scale the point to fit on the original image\n","#     x = (frameWidth * point[0]) / W\n","#     y = (frameHeight * point[1]) / H\n","\n","#     if prob > threshold : \n","#         cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n","#         cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n","#         cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n","\n","#         # Add the point to the list if the probability is greater than the threshold\n","#         points.append((int(x), int(y)))\n","#     else :\n","#         points.append(None)\n","\n","# # Draw Skeleton\n","# for pair in POSE_PAIRS:\n","#     partA = pair[0]\n","#     partB = pair[1]\n","\n","#     if points[partA] and points[partB]:\n","#         cv2.line(frame, points[partA], points[partB], (0, 255, 255), 3)\n","\n","# plt.figure(figsize=[10,10])\n","# plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))\n","# plt.figure(figsize=[10,10])\n","# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","# cv2.imwrite(f'/Users/viduth/Desktop/stanceout/image.{count}.png', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2fiiF5vXzWb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34zQfHDWXzWb"},"outputs":[],"source":[""]}],"metadata":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"name":"OpenPose_Notebook.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}